[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "",
    "section": "",
    "text": "Google Scholar Profile\nNote: Please reach out to me over e-mail for PDFs not attached here/broken links"
  },
  {
    "objectID": "publications.html#section",
    "href": "publications.html#section",
    "title": "",
    "section": "2023",
    "text": "2023\n\nTowards Scalable Identification of Brick Kilns from Satellite Imagery with Active Learning. Aditi Agarwal, Suraj Jaiswal, Madhav Kanda, Dhruv Patel, Rishabh Mondal, Vannsh Jani, Zeel B Patel, Nipun Batra and Sarath Guttikunda, In RealWorldML workshop, NeurIPS 2023 [PDF]\nFine-Grained Spatio-Temporal Particulate Matter Dataset From Delhi For ML based Modeling. Sachin Chauhan, Zeel B Patel, Sayan Ranu, Rijurekha Sen, Nipun Batra, In NeurIPS 2023 (Datasets and Benchmarks). [PDF]\nConformal Prediction: A Visual Introduction. Mihir Agarwal, Lalit Routhu, Zeel B Patel, Nipun Batra, In VISxAI workshop 2023, IEEE VIS. [URL]\nDeep Gaussian Processes for Air Quality Inference. Aadesh Desai, Eshan Gujarathi, Saagar Parikh, Sachin Yadav, Zeel Patel, Nipun Batra. In CODS-COMAD 2023 Young Researchers Symposium track."
  },
  {
    "objectID": "publications.html#section-1",
    "href": "publications.html#section-1",
    "title": "",
    "section": "2022",
    "text": "2022\n\nUncertainty Disentanglement with Non-stationary Heteroscedastic Gaussian Processes for Active Learning. Zeel B Patel, Nipun Batra, Kevin Murphy. In NeurIPS Workshop on Gaussian Processes, Spatiotemporal Modeling, and Decision-making Systems 2022 [arXiv]\nChallenges in Gaussian Processes for Non Intrusive Load Monitoring. Aadesh Desai, Gautam Vashishtha, Zeel B Patel, Nipun Batra. In NeurIPS Workshop on Gaussian Processes, Spatiotemporal Modeling, and Decision-making Systems 2022 [arXiv]\nSamachar: Print News Media on Air Pollution in India. Karm Patel, Rishiraj Adhikary, Zeel B Patel, Nipun Batra, and Sarath Guttikunda. In ACM SIGCAS Computing and Sustainable Societies (COMPASS ’22) [Publisher Link]\nAccurate and Scalable Gaussian Processes for Fine-grained Air Quality Inference. Zeel B Patel, Palak Purohit, Harsh Patel, Shivam Sahni, and Nipun Batra. AAAI 2022, Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22) (CORE A* with 15% acceptance rate) [PDF] [GitHub]"
  },
  {
    "objectID": "publications.html#section-2",
    "href": "publications.html#section-2",
    "title": "",
    "section": "2021",
    "text": "2021\n\nTowards Active Air Quality Station Deployment. Zeel B Patel, Nipun Batra. SubSetML Workshop, ICML 2021 (CORE A*) [PDF]\nVartalaap: what drives #AirQuality discussions: politics, pollution or pseudo-science? Rishiraj Adhikary, Zeel B Patel, Tanmay Srivastava, Nipun Batra, Mayank Singh, Udit Bhatia, and Sarath Guttikunda. ACM CSCW 2021 (CORE A) [PDF] [DOI]"
  },
  {
    "objectID": "publications.html#section-3",
    "href": "publications.html#section-3",
    "title": "",
    "section": "2020",
    "text": "2020\n\nPoster: A toolkit for spatial interpolation and sensor placement, Zeel B Patel, S Deepak Narayanan, Apoorv Agnihotri, Nipun Batra. ACM SenSys 2020 (CORE A*)[PDF] [DOI]\nActive Learning: A Visual Tour, Zeel B Patel, Nipun Batra. VisXAI Workshop, IEEE VIS 2020 (CORE A) [URL]"
  },
  {
    "objectID": "timeline.html",
    "href": "timeline.html",
    "title": "",
    "section": "",
    "text": "2024 Jun 17\n\n\nAchievement\n\n\nReceived Microsoft Research India PhD Fellowship\n\n\n\n\n2023 Dec 10-16\n\n\nConference\n\n\nAttended NeurIPS conference in-person at New Orleans, USA\n\n\n\n\n2023 Oct 27\n\n\nPublication\n\n\nA workshop paper titled Towards Scalable Identification of Brick Kilns from Satellite Imagery with Active Learning got accepted in NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World\n\n\n\n\n2023 Sep 22\n\n\nPublication\n\n\nA paper titled Fine-Grained Spatio-Temporal Particulate Matter Dataset From Delhi For ML based Modeling got accepted in NeurIPS 2023 Datasets and Benchmarks Track\n\n\n\n\n2023 Sep 11\n\n\nPublication\n\n\nA visual explainable Conformal Prediction: A Visual Introduction got accepted at VisXAI.\n\n\n\n\n2023 Aug 25\n\n\nService\n\n\nReviewer at AAAI 2024\n\n\n\n\n2023 Jan 29-31\n\n\nConference\n\n\nAttended Google Research Week at Bangalore, India\n\n\n\n\n2022 Oct 19\n\n\nService\n\n\nReviewer at AISTATS 2023\n\n\n\n\n2022 Oct 14\n\n\nPublication\n\n\nTwo workshop papers got accepted at NeurIPS Workshop on Gaussian Processes, Spatiotemporal Modeling, and Decision-making Systems\n\n\n\n\n2022 Aug 16\n\n\nConference\n\n\nPresented an invited talk at ICAS-ASIC conference in Bengalore organized by CSTEP and UC Davis Air Quality Research Center\n\n\n\n\n2022 May 21\n\n\nAchievement\n\n\nSelected as a Google Summer of Code contributor at TensorFlow [Proposal] [Final Report]  Mentors: Kevin Murphy and Nipun Batra\n\n\n\n\n2022 Apr 26\n\n\nPublication\n\n\nOur work Samachar: Print News Media on Air Pollution in India is accepted in ACM COMPASS 2022\n\n\n\n\n2022 Feb 8-11\n\n\nConference\n\n\nAttended Google Research Week virtually\n\n\n\n\n2021 Dec 2\n\n\nPublication\n\n\nOur work Accurate and Scalable Gaussian Processes for Fine-grained Air Quality Inference is accepted at AAAI 22 (15% acceptance rate)\n\n\n\n\n2021 Sep 13-16\n\n\nConference\n\n\nAttended Gaussian Process Summer School virtually\n\n\n\n\n2021 Jun 18\n\n\nPublication\n\n\nA paper titled “Towards Active Air Quality Station Deployment” accepted in SubSetML workshop in ICML 2021\n\n\n\n\n2021 Jun 7\n\n\nService\n\n\nReviewer at ACM COMPASS 2021 Posters and Demos\n\n\n\n\n2021 Apr 7-10\n\n\nConference\n\n\nAttended Google Graduate Symposium 2021 virtually\n\n\n\n\n2021 Mar 8\n\n\nMilestone\n\n\nCleared PhD Qualifiers Exam\n\n\n\n\n2020 Dec 22\n\n\nPublication\n\n\nOur work Vartalaap: What Drives #AirQuality Discussions: Politics,Pollution or Pseudo-science? is accepted and published in ACM CSCW 2021\n\n\n\n\n2020 Oct 15\n\n\nPublication\n\n\nA poster A toolkit for spatial interpolation and sensor placement is accepted in ACM SenSys 2020\n\n\n\n\n2020 Sep 14-17\n\n\nConference\n\n\nAttended Gaussian Process Summer School virtually\n\n\n\n\n2020 Aug 23-27\n\n\nConference\n\n\nAttended KDD conference virtually\n\n\n\n\ntheme is inspired by"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zeel B Patel",
    "section": "",
    "text": "Zeel is a PhD student in Computer Science and Engineering at the Sustainability lab, IIT Gandhinagar advised by Prof. Nipun Batra. His research area of interest is “AI for Social Good”.\n\nKey Achievements\n\nZeel is a Microsoft Research India PhD Fellow.\nZeel was a Google Summer of Code (GSoC) contributor at TensorFlow in 2022.\nZeel is a co-author in Probabilistic Machine Learning: Advanced Topics () book authored by a well-known author Kevin Murphy."
  },
  {
    "objectID": "gsoc/gsoc22.html",
    "href": "gsoc/gsoc22.html",
    "title": "GSoC 2022 - Final Work Report",
    "section": "",
    "text": "In this gist, I present my work during the Google Summer of Code (GSoC) 2022 program. In the project, I contributed code to a published book Probabilistic Machine Learning: An Introduction (book1) and an upcoming book Probabilistic Machine Learning: Advanced Topics (book2) by Kevin P. Murphy.\nI mainly worked on two repositories: i) pyprobml (public): a repositiry containing most of the source code for the book; and ii) bookv2 (private): a repository containing source code of the book.\n\n\n\nI worked on this project for 16 weeks and on an average 42 hours a week.\nI refactored the pyprobml repository to shift from .py files to .ipynb files containing code to reproduce figures.\nI added automatic code testing via GitHub workflows to pyprobml.\nI generated code for several figures in “Gaussian processes” chapter.\nI developed a Bayesian optimization library in JAX called BIJAX.\nI co-authored a section (34.7 - Active learning) with Dr. Kevin P. Murphy.\nI learned advanced tools of JAX library.\n\n\n\n\n\n\nIn earlier version of repo, code for each figure was stored in .py files and there were Jupyter notebooks, each per chapter, containing cells to execute each of those .py files. Link to those cells were stored in a private firestore database and a short link was given with each figure in book1.\nThere were several challanges with this setup: * It was difficult to maintain and keep this setup up to date due to manual intervention needed at multiple places. * Due to full .py file execution, interactivity with code was minimal.\n\n\n\nAfter brainstorming, we decided to move from .py file per figure to a self-contained .ipynb file per figure which alleviated the need of notebook per chapter. This also enabled readers to play with the notebook more interactively or view the cached figures in the notebook. In the final version, we have notebooks stored in folders renamed as chapter numbers and integrated with book without the need of firestore database.\n\n\n\n\n\n\nCode for any repository needs be up to date with packages it depends on. pyprobml depends on lots of packages and thus automatic GitHub workflow testing can be beneficial to test the code on every push.\n\n\n\nI implemented majority part of workflow which: i) runs and chekcks all notebooks on every push; ii) statically checks if imported modules are installed/going to be installed locally or not; iii) checks code quality with black and iv) pushes some useful statistic to other branches e.g. auto_generated_figure. Following PRs are major in making these changes:\n\n\n\nURL\nTitle\n\n\n\n\nPR 703\nAdd workflow and jupyter-book setup\n\n\nPR 764\nSplit the workflow. Enable static module checker.\n\n\nPR 795\nCombine and publish results from parallel workflows\n\n\n\n\n\n\n\nGaussian processes are related to my thesis topic and thus I wrote code and/or latexified several figures in the Gaussian processes chapter. Here is the list of figures generated by my code:\n\nFigure 18.2 [PR 727] explains ARD in Gaussian processes: a) same lengthscale for both dimensions; ii) lengthscale in one dimension dominates the other.\n\n\n\n\n\nFigure 18.5 and 18.6 [PR 721, PR 722] illustrates combination of GP kernels via multiplication and summation\n\n\n\n\n\n\n\n\nFigure 18.7 [PR 728, PR 750] illustrates noisy and noiseless Gaussian process regression: a) samples from prior; b) noiseless regression; c) noisy regression.\n\n\n\n\n\nFigure 18.18 [PR 919] illustrates GP regression with (a) Maximum likelihood and (b) Fully Bayesian inference. I added blackjax version of code for this.\n\n\n\n\n\nFigure 18.23 [PR 874] illustrates spectral mixture kernel fit with GPs. I latexified this figure.\n\n\n\n\n\nFigure 18.26 [PR 871] illustrates Deep kernel learning with Gaussian processes: a) fit with Matern kernel; b) fit with MLP + Matern kernel. I latexifed this figure.\n\n\n\n\n\n\n\nWe initially thought of writing a generic piece of code to generate multiple Bayesian inference related figures in book2. However, realizing the power of Bijactors (transformation functions) in recent methods like ADVI and a posibility of applications for a wider community, we are developing BIJAX as a full library. BIJAX supports MAP, ADVI and Laplace approximation. We are developing SteinVI and other methods in BIJAX at present.\nFollowing are the figures developed with BIJAX:\n\nGaussian Mixture Model with ADVI [PR 1084]: a) MAP; b-d) posterior samples.\n\n\n\n\n\nEstimating mean of 2d Gaussian distribution with ADVI [PR 1081]\n\n\n\n\n\n\n\nWe work on active learning in our lab and earlier I have published an interactive active learning article at VizxAI. Thus, I helped Dr. Murphy write the section on active learning in book2. I mentored two of non-GSoC students (Nitish and Ankita) from our lab at IIT Gandhinagar for generating all figures in this section.\n\n\n\n\n\n\nDuring GSoC, I got introduced to JAX for the first time. After some exploration, I found JAX as a quick and computationally fast prototyping library. Tools like vmap, jit, tree_util and flatten_util were elementry in the code I wrote during GSoC. I summarized the JAX tricks we learned during the GSoC period here.\n\n\n\nFollowing table containts the list of PRs created during the GSoC period sorted by time.\n\n\n\nURL\nTitle\n\n\n\n\nPR 690\nUpdate beta_binom_post_plot.py\n\n\nPR 703\nAdd workflow and jupyter-book setup\n\n\nPR 704\nAdded discrete_prob_dist_plot.ipynb\n\n\nPR 707\nDo not auto generate scripts\n\n\nPR 709\nRename environment variables\n\n\nPR 719\nRemove jupyter book\n\n\nPR 721\nadd combining_kernels_by_multiplication notebook\n\n\nPR 722\nadd combining_kernels_by_summation notebook\n\n\nPR 727\nFig18.2 (Book2) add gprDemoArd.ipynb\n\n\nPR 728\nFig18.7 (Book2) add gprDemoNoiseFreeAndNoisy.ipynb\n\n\nPR 741\nconverted py files to notebooks.\n\n\nPR 750\nUpdate gprDemoNoiseFreeAndNoisy\n\n\nPR 763\nMention Python version in README\n\n\nPR 764\nSplit the workflow. Enable static module checker.\n\n\nPR 779\npin Python 3.7.13 in workflow\n\n\nPR 780\nCopied d2l notebooks to pyprobml\n\n\nPR 787\nMove non-figure notebooks to pyprobml\n\n\nPR 795\nCombine and publish results from parallel workflows\n\n\nPR 798\nUpdated README.md and CONTRIBUTING.md\n\n\nPR 800\nCopy probml-notebooks/notebooks to notebooks/misc\n\n\nPR 805\nPush updated notebooks to misc\n\n\nPR 824\nAdd book2 notebooks\n\n\nPR 834\nGitHub to colab\n\n\nPR 846\nAdd autogenerated contributors’ list\n\n\nPR 871\nRefactor deep kernel learning figure\n\n\nPR 874\nFix workflow and latexify spectral mixture gp figures\n\n\nPR 919\nLatexified and blackjaxified figure 17.18 gp_kernel_opt.ipynb\n\n\nPR 1046\nAL notebooks added\n\n\nPR 1081\nadded gaussian 2d ADVI\n\n\nPR 1084\nAdded GMM ADVI notebook\n\n\nPR 1099\nLatexify vib_demo\n\n\n\n\n\n\nThis GSoC I worked on a big project useful to the academic community around the world and with a wonderful mentor Dr. Kevin Murphy. Simultaniously, I learned my new default ML library JAX and increased my confidence to be able to work on such large projects. I will always be grateful to Dr. Kevin Murphy for this opportunity that helped me add a unique and valuable experience in my PhD journy."
  },
  {
    "objectID": "gsoc/gsoc22.html#highlights",
    "href": "gsoc/gsoc22.html#highlights",
    "title": "GSoC 2022 - Final Work Report",
    "section": "",
    "text": "I worked on this project for 16 weeks and on an average 42 hours a week.\nI refactored the pyprobml repository to shift from .py files to .ipynb files containing code to reproduce figures.\nI added automatic code testing via GitHub workflows to pyprobml.\nI generated code for several figures in “Gaussian processes” chapter.\nI developed a Bayesian optimization library in JAX called BIJAX.\nI co-authored a section (34.7 - Active learning) with Dr. Kevin P. Murphy.\nI learned advanced tools of JAX library."
  },
  {
    "objectID": "gsoc/gsoc22.html#py-to-.ipynb-conversion",
    "href": "gsoc/gsoc22.html#py-to-.ipynb-conversion",
    "title": "GSoC 2022 - Final Work Report",
    "section": "",
    "text": "In earlier version of repo, code for each figure was stored in .py files and there were Jupyter notebooks, each per chapter, containing cells to execute each of those .py files. Link to those cells were stored in a private firestore database and a short link was given with each figure in book1.\nThere were several challanges with this setup: * It was difficult to maintain and keep this setup up to date due to manual intervention needed at multiple places. * Due to full .py file execution, interactivity with code was minimal.\n\n\n\nAfter brainstorming, we decided to move from .py file per figure to a self-contained .ipynb file per figure which alleviated the need of notebook per chapter. This also enabled readers to play with the notebook more interactively or view the cached figures in the notebook. In the final version, we have notebooks stored in folders renamed as chapter numbers and integrated with book without the need of firestore database."
  },
  {
    "objectID": "gsoc/gsoc22.html#automatic-code-testing",
    "href": "gsoc/gsoc22.html#automatic-code-testing",
    "title": "GSoC 2022 - Final Work Report",
    "section": "",
    "text": "Code for any repository needs be up to date with packages it depends on. pyprobml depends on lots of packages and thus automatic GitHub workflow testing can be beneficial to test the code on every push.\n\n\n\nI implemented majority part of workflow which: i) runs and chekcks all notebooks on every push; ii) statically checks if imported modules are installed/going to be installed locally or not; iii) checks code quality with black and iv) pushes some useful statistic to other branches e.g. auto_generated_figure. Following PRs are major in making these changes:\n\n\n\nURL\nTitle\n\n\n\n\nPR 703\nAdd workflow and jupyter-book setup\n\n\nPR 764\nSplit the workflow. Enable static module checker.\n\n\nPR 795\nCombine and publish results from parallel workflows"
  },
  {
    "objectID": "gsoc/gsoc22.html#figures-for-gaussian-processes-chapter",
    "href": "gsoc/gsoc22.html#figures-for-gaussian-processes-chapter",
    "title": "GSoC 2022 - Final Work Report",
    "section": "",
    "text": "Gaussian processes are related to my thesis topic and thus I wrote code and/or latexified several figures in the Gaussian processes chapter. Here is the list of figures generated by my code:\n\nFigure 18.2 [PR 727] explains ARD in Gaussian processes: a) same lengthscale for both dimensions; ii) lengthscale in one dimension dominates the other.\n\n\n\n\n\nFigure 18.5 and 18.6 [PR 721, PR 722] illustrates combination of GP kernels via multiplication and summation\n\n\n\n\n\n\n\n\nFigure 18.7 [PR 728, PR 750] illustrates noisy and noiseless Gaussian process regression: a) samples from prior; b) noiseless regression; c) noisy regression.\n\n\n\n\n\nFigure 18.18 [PR 919] illustrates GP regression with (a) Maximum likelihood and (b) Fully Bayesian inference. I added blackjax version of code for this.\n\n\n\n\n\nFigure 18.23 [PR 874] illustrates spectral mixture kernel fit with GPs. I latexified this figure.\n\n\n\n\n\nFigure 18.26 [PR 871] illustrates Deep kernel learning with Gaussian processes: a) fit with Matern kernel; b) fit with MLP + Matern kernel. I latexifed this figure."
  },
  {
    "objectID": "gsoc/gsoc22.html#bijax-bayesian-inference-in-jax",
    "href": "gsoc/gsoc22.html#bijax-bayesian-inference-in-jax",
    "title": "GSoC 2022 - Final Work Report",
    "section": "",
    "text": "We initially thought of writing a generic piece of code to generate multiple Bayesian inference related figures in book2. However, realizing the power of Bijactors (transformation functions) in recent methods like ADVI and a posibility of applications for a wider community, we are developing BIJAX as a full library. BIJAX supports MAP, ADVI and Laplace approximation. We are developing SteinVI and other methods in BIJAX at present.\nFollowing are the figures developed with BIJAX:\n\nGaussian Mixture Model with ADVI [PR 1084]: a) MAP; b-d) posterior samples.\n\n\n\n\n\nEstimating mean of 2d Gaussian distribution with ADVI [PR 1081]"
  },
  {
    "objectID": "gsoc/gsoc22.html#co-authored-section-34.7---active-learning",
    "href": "gsoc/gsoc22.html#co-authored-section-34.7---active-learning",
    "title": "GSoC 2022 - Final Work Report",
    "section": "",
    "text": "We work on active learning in our lab and earlier I have published an interactive active learning article at VizxAI. Thus, I helped Dr. Murphy write the section on active learning in book2. I mentored two of non-GSoC students (Nitish and Ankita) from our lab at IIT Gandhinagar for generating all figures in this section."
  },
  {
    "objectID": "gsoc/gsoc22.html#jax-an-ml-library-in-sync-with-pure-python",
    "href": "gsoc/gsoc22.html#jax-an-ml-library-in-sync-with-pure-python",
    "title": "GSoC 2022 - Final Work Report",
    "section": "",
    "text": "During GSoC, I got introduced to JAX for the first time. After some exploration, I found JAX as a quick and computationally fast prototyping library. Tools like vmap, jit, tree_util and flatten_util were elementry in the code I wrote during GSoC. I summarized the JAX tricks we learned during the GSoC period here."
  },
  {
    "objectID": "gsoc/gsoc22.html#summary-of-prs",
    "href": "gsoc/gsoc22.html#summary-of-prs",
    "title": "GSoC 2022 - Final Work Report",
    "section": "",
    "text": "Following table containts the list of PRs created during the GSoC period sorted by time.\n\n\n\nURL\nTitle\n\n\n\n\nPR 690\nUpdate beta_binom_post_plot.py\n\n\nPR 703\nAdd workflow and jupyter-book setup\n\n\nPR 704\nAdded discrete_prob_dist_plot.ipynb\n\n\nPR 707\nDo not auto generate scripts\n\n\nPR 709\nRename environment variables\n\n\nPR 719\nRemove jupyter book\n\n\nPR 721\nadd combining_kernels_by_multiplication notebook\n\n\nPR 722\nadd combining_kernels_by_summation notebook\n\n\nPR 727\nFig18.2 (Book2) add gprDemoArd.ipynb\n\n\nPR 728\nFig18.7 (Book2) add gprDemoNoiseFreeAndNoisy.ipynb\n\n\nPR 741\nconverted py files to notebooks.\n\n\nPR 750\nUpdate gprDemoNoiseFreeAndNoisy\n\n\nPR 763\nMention Python version in README\n\n\nPR 764\nSplit the workflow. Enable static module checker.\n\n\nPR 779\npin Python 3.7.13 in workflow\n\n\nPR 780\nCopied d2l notebooks to pyprobml\n\n\nPR 787\nMove non-figure notebooks to pyprobml\n\n\nPR 795\nCombine and publish results from parallel workflows\n\n\nPR 798\nUpdated README.md and CONTRIBUTING.md\n\n\nPR 800\nCopy probml-notebooks/notebooks to notebooks/misc\n\n\nPR 805\nPush updated notebooks to misc\n\n\nPR 824\nAdd book2 notebooks\n\n\nPR 834\nGitHub to colab\n\n\nPR 846\nAdd autogenerated contributors’ list\n\n\nPR 871\nRefactor deep kernel learning figure\n\n\nPR 874\nFix workflow and latexify spectral mixture gp figures\n\n\nPR 919\nLatexified and blackjaxified figure 17.18 gp_kernel_opt.ipynb\n\n\nPR 1046\nAL notebooks added\n\n\nPR 1081\nadded gaussian 2d ADVI\n\n\nPR 1084\nAdded GMM ADVI notebook\n\n\nPR 1099\nLatexify vib_demo"
  },
  {
    "objectID": "gsoc/gsoc22.html#summary",
    "href": "gsoc/gsoc22.html#summary",
    "title": "GSoC 2022 - Final Work Report",
    "section": "",
    "text": "This GSoC I worked on a big project useful to the academic community around the world and with a wonderful mentor Dr. Kevin Murphy. Simultaniously, I learned my new default ML library JAX and increased my confidence to be able to work on such large projects. I will always be grateful to Dr. Kevin Murphy for this opportunity that helped me add a unique and valuable experience in my PhD journy."
  }
]